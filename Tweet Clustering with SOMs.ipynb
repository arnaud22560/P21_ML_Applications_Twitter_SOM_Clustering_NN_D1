{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering tweets about Machine Learning using self-organizing maps\n",
    "\n",
    "*Arnaud Le Doeuff*\n",
    "*Ignacio Dorado*\n",
    "*11/20*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/RodolfoFerro/pandas_twitter/blob/master/01-extracting-data.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tweepy\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Capture the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about twitter api, credentials and stafff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *    # This will allow us to use the keys as variables\n",
    "\n",
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with our access keys provided.\n",
    "    \"\"\"\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    return api\n",
    "\n",
    "# We create an extractor object:\n",
    "extractor = twitter_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter will only allow us to download 3200 tweets every 15 minutes, which is not a lot considering most of them are retweeets. Only 7 days (or so) can be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words we want to search for\n",
    "searchQuery = \"machine learning\"\n",
    "\n",
    "# Maximum number of tweets we want to collect \n",
    "maxTweets = 10000\n",
    "\n",
    "# Show our current limitations\n",
    "extractor.rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 0 tweets\n",
      "Saw 0 tweets\n",
      "Downloaded 188 tweets\n",
      "Saw 1000 tweets\n",
      "Downloaded 371 tweets\n",
      "Saw 2000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 484 tweets\n",
      "Saw 3000 tweets\n",
      "Downloaded 568 tweets\n",
      "Saw 4000 tweets\n",
      "Downloaded 682 tweets\n",
      "Saw 5000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 771 tweets\n",
      "Saw 6000 tweets\n",
      "Downloaded 865 tweets\n",
      "Saw 7000 tweets\n",
      "Downloaded 972 tweets\n",
      "Saw 8000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1069 tweets\n",
      "Saw 9000 tweets\n",
      "Downloaded 1170 tweets\n",
      "Saw 10000 tweets\n"
     ]
    }
   ],
   "source": [
    "tweetCount = 0\n",
    "global_count = 0\n",
    "# We create a tweet list as follows:\n",
    "tweets=[]\n",
    "#Tell the Cursor method that we want to use the Search API (api.search)\n",
    "#Also tell Cursor our query, and the maximum number of tweets to return\n",
    "for tweet in tweepy.Cursor(extractor.search,q=searchQuery).items(maxTweets):\n",
    "    \n",
    "    #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "    if not tweet.text.startswith(\"RT \"):\n",
    "            tweets.append(tweet)\n",
    "            tweetCount += 1\n",
    "    global_count += 1 \n",
    "    \n",
    "    if (global_count%1000 == 0):\n",
    "        print(\"Downloaded {0} tweets\".format(global_count))\n",
    "        print(\"Kept {0} non RT tweets\".format(tweetCount))\n",
    "    \n",
    "\n",
    "#Display how many tweets we have collected\n",
    "print(\"Downloaded {0} tweets\".format(global_count))\n",
    "print(\"Kept {0} non RT tweets\".format(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tweets: <class 'list'>\n",
      "Type of each tweet: <class 'tweepy.models.Status'>\n"
     ]
    }
   ],
   "source": [
    "print (\"Type of tweets: \" + str(type(tweets)))\n",
    "print (\"Type of each tweet: \" + str(type(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_api', '_json', 'author', 'contributors', 'coordinates', 'created_at', 'destroy', 'entities', 'favorite', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'metadata', 'parse', 'parse_list', 'place', 'possibly_sensitive', 'retweet', 'retweet_count', 'retweeted', 'retweets', 'source', 'source_url', 'text', 'truncated', 'user']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>len</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Likes</th>\n",
       "      <th>RTs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ü§ù NEW PARTNERSHIP ü§ù \\n\\nAt @Fetch_ai we are su...</td>\n",
       "      <td>137</td>\n",
       "      <td>1329002221615407104</td>\n",
       "      <td>2020-11-18 10:03:15</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@elonmusk Do you believe machine learning is s...</td>\n",
       "      <td>109</td>\n",
       "      <td>1329002220118024193</td>\n",
       "      <td>2020-11-18 10:03:15</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eyes on $FET\\n\\nhttps://t.co/Vu1StthG4d partne...</td>\n",
       "      <td>140</td>\n",
       "      <td>1329002190942429184</td>\n",
       "      <td>2020-11-18 10:03:08</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Ways Machine Learning Will Reshape Manufact...</td>\n",
       "      <td>94</td>\n",
       "      <td>1329002168918167553</td>\n",
       "      <td>2020-11-18 10:03:03</td>\n",
       "      <td>Upflow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Strands is a dynamic company that adopts a di...</td>\n",
       "      <td>140</td>\n",
       "      <td>1329001924230844421</td>\n",
       "      <td>2020-11-18 10:02:05</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://t.co/3RAep6sWsQ  \\n  If you have any q...</td>\n",
       "      <td>137</td>\n",
       "      <td>1328994433585401859</td>\n",
       "      <td>2020-11-18 09:32:19</td>\n",
       "      <td>boutlineprod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://t.co/3RAep6sWsQ  \\n  Our digital produ...</td>\n",
       "      <td>106</td>\n",
       "      <td>1328994433145081858</td>\n",
       "      <td>2020-11-18 09:32:19</td>\n",
       "      <td>boutlineprod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://t.co/3RAep6sWsQ  \\n  What's Included W...</td>\n",
       "      <td>140</td>\n",
       "      <td>1328994432624914432</td>\n",
       "      <td>2020-11-18 09:32:18</td>\n",
       "      <td>boutlineprod</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Solid thread on sniff-testing machine learning...</td>\n",
       "      <td>80</td>\n",
       "      <td>1328994413813538817</td>\n",
       "      <td>2020-11-18 09:32:14</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>‚ÄòThe ability to deploy a swarm of low-cost aut...</td>\n",
       "      <td>140</td>\n",
       "      <td>1328994222003920897</td>\n",
       "      <td>2020-11-18 09:31:28</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets  len  \\\n",
       "0   ü§ù NEW PARTNERSHIP ü§ù \\n\\nAt @Fetch_ai we are su...  137   \n",
       "1   @elonmusk Do you believe machine learning is s...  109   \n",
       "2   Eyes on $FET\\n\\nhttps://t.co/Vu1StthG4d partne...  140   \n",
       "3   10 Ways Machine Learning Will Reshape Manufact...   94   \n",
       "4   #Strands is a dynamic company that adopts a di...  140   \n",
       "..                                                ...  ...   \n",
       "95  https://t.co/3RAep6sWsQ  \\n  If you have any q...  137   \n",
       "96  https://t.co/3RAep6sWsQ  \\n  Our digital produ...  106   \n",
       "97  https://t.co/3RAep6sWsQ  \\n  What's Included W...  140   \n",
       "98  Solid thread on sniff-testing machine learning...   80   \n",
       "99  ‚ÄòThe ability to deploy a swarm of low-cost aut...  140   \n",
       "\n",
       "                     ID                Date              Source  Likes  RTs  \n",
       "0   1329002221615407104 2020-11-18 10:03:15     Twitter Web App      0    0  \n",
       "1   1329002220118024193 2020-11-18 10:03:15  Twitter for iPhone      0    0  \n",
       "2   1329002190942429184 2020-11-18 10:03:08     Twitter Web App      0    0  \n",
       "3   1329002168918167553 2020-11-18 10:03:03              Upflow      0    0  \n",
       "4   1329001924230844421 2020-11-18 10:02:05             HubSpot      0    0  \n",
       "..                  ...                 ...                 ...    ...  ...  \n",
       "95  1328994433585401859 2020-11-18 09:32:19        boutlineprod      0    0  \n",
       "96  1328994433145081858 2020-11-18 09:32:19        boutlineprod      0    0  \n",
       "97  1328994432624914432 2020-11-18 09:32:18        boutlineprod      0    1  \n",
       "98  1328994413813538817 2020-11-18 09:32:14  Twitter for iPhone      0    1  \n",
       "99  1328994222003920897 2020-11-18 09:31:28  Twitter for iPhone      0    1  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We create a pandas dataframe as follows:\n",
    "df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "#We add all the information we want to keep about the tweets\n",
    "df['len']  = np.array([len(tweet.text) for tweet in tweets])\n",
    "df['ID']   = np.array([tweet.id for tweet in tweets])\n",
    "df['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "df['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "df['Likes']  = np.array([tweet.favorite_count for tweet in tweets])\n",
    "df['RTs']    = np.array([tweet.retweet_count for tweet in tweets])\n",
    "\n",
    "# We display the first 10 elements of the dataframe:\n",
    "display(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the Data Frame it to a csv file so we can read it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First thing would be to get rid off every comma, point and any other strange symbol\n",
    "- Second thing would be creating the dictionary\n",
    "- Then reducing the dictionay\n",
    "- Finally do some PCA (probabbly(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discuss the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
