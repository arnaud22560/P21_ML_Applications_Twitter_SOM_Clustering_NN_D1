{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering tweets about Machine Learning using self-organizing maps\n",
    "\n",
    "*Arnaud Le Doeuff*\n",
    "*Ignacio Dorado*\n",
    "*11/20*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/RodolfoFerro/pandas_twitter/blob/master/01-extracting-data.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tweepy\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Capture the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about twitter api, credentials and stafff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *    # This will allow us to use the keys as variables\n",
    "\n",
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with our access keys provided.\n",
    "    \"\"\"\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    return api\n",
    "\n",
    "# We create an extractor object:\n",
    "extractor = twitter_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter will only allow us to download 3200 tweets every 15 minutes, which is not a lot considering most of them are retweeets. Only 7 days (or so) can be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 180, 'remaining': 180, 'reset': 1605742854}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words we want to search for\n",
    "searchQuery = \"machine learning\"\n",
    "\n",
    "# Maximum number of tweets we want to collect \n",
    "maxTweets = 10000\n",
    "\n",
    "# Show our current limitations\n",
    "extractor.rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1000 tweets\n",
      "Kept 238 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 2000 tweets\n",
      "Kept 450 non RT tweets\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3000 tweets\n",
      "Kept 638 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 4000 tweets\n",
      "Kept 882 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 5000 tweets\n",
      "Kept 1102 non RT tweets\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 6000 tweets\n",
      "Kept 1307 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 7000 tweets\n",
      "Kept 1578 non RT tweets\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 8000 tweets\n",
      "Kept 1827 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 9000 tweets\n",
      "Kept 2081 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 10000 tweets\n",
      "Kept 2302 non RT tweets\n",
      "---------------------------\n",
      "Downloaded 10000 tweets\n",
      "Kept 2302 non RT tweets\n"
     ]
    }
   ],
   "source": [
    "tweetCount = 0\n",
    "global_count = 0\n",
    "# We create a tweet list as follows:\n",
    "tweets=[]\n",
    "#Tell the Cursor method that we want to use the Search API (api.search)\n",
    "#Also tell Cursor our query, and the maximum number of tweets to return\n",
    "for tweet in tweepy.Cursor(extractor.search,q=searchQuery, tweet_mode='extended').items(maxTweets):\n",
    "    \n",
    "    #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "    if not tweet.full_text.startswith(\"RT \"):\n",
    "            tweets.append(tweet)\n",
    "            tweetCount += 1\n",
    "    global_count += 1 \n",
    "    \n",
    "    if (global_count%1000 == 0):\n",
    "        print(\"Downloaded {0} tweets\".format(global_count))\n",
    "        print(\"Kept {0} non RT tweets\".format(tweetCount))\n",
    "        print(\"---------------------------\")\n",
    "    \n",
    "\n",
    "#Display how many tweets we have collected\n",
    "print(\"Downloaded {0} tweets\".format(global_count))\n",
    "print(\"Kept {0} non RT tweets\".format(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tweets: <class 'list'>\n",
      "Type of each tweet: <class 'tweepy.models.Status'>\n"
     ]
    }
   ],
   "source": [
    "print (\"Type of tweets: \" + str(type(tweets)))\n",
    "print (\"Type of each tweet: \" + str(type(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_api', '_json', 'author', 'contributors', 'coordinates', 'created_at', 'destroy', 'display_text_range', 'entities', 'extended_entities', 'favorite', 'favorite_count', 'favorited', 'full_text', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'metadata', 'parse', 'parse_list', 'place', 'possibly_sensitive', 'retweet', 'retweet_count', 'retweeted', 'retweets', 'source', 'source_url', 'truncated', 'user']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>len</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Likes</th>\n",
       "      <th>RTs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SICK’s Deep Learning brings simplicity to complex AI ins...</td>\n",
       "      <td>303</td>\n",
       "      <td>1329334610946260992</td>\n",
       "      <td>2020-11-19 08:04:03</td>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Cloud Debuts Professional Machine Learning Engine...</td>\n",
       "      <td>103</td>\n",
       "      <td>1329334598455734277</td>\n",
       "      <td>2020-11-19 08:04:00</td>\n",
       "      <td>Paper.li</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bringing your own custom container image to Amazon SageM...</td>\n",
       "      <td>142</td>\n",
       "      <td>1329334594085269506</td>\n",
       "      <td>2020-11-19 08:03:59</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here’s what machines need to understand in order to trul...</td>\n",
       "      <td>238</td>\n",
       "      <td>1329334592449507331</td>\n",
       "      <td>2020-11-19 08:03:59</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Level up your data science vocabulary: Geometric Distrib...</td>\n",
       "      <td>121</td>\n",
       "      <td>1329334535532670977</td>\n",
       "      <td>2020-11-19 08:03:45</td>\n",
       "      <td>DeepAI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5 Most Useful Machine Learning Tools every lazy full-sta...</td>\n",
       "      <td>124</td>\n",
       "      <td>1329322590670893059</td>\n",
       "      <td>2020-11-19 07:16:17</td>\n",
       "      <td>Paper.li</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The way we train AI is fundamentally flawed https://t.co...</td>\n",
       "      <td>67</td>\n",
       "      <td>1329322554541154306</td>\n",
       "      <td>2020-11-19 07:16:09</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Machine Learning: MLflow 1.12 verbessert die PyTorch-Int...</td>\n",
       "      <td>100</td>\n",
       "      <td>1329322407811801088</td>\n",
       "      <td>2020-11-19 07:15:34</td>\n",
       "      <td>Paper.li</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Your fleet is in safe hands with #JupiCar, an innovative...</td>\n",
       "      <td>296</td>\n",
       "      <td>1329322279893950464</td>\n",
       "      <td>2020-11-19 07:15:03</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>@Tata_Crucible A. Machine Learning\\n\\n#TechTalk #TataCru...</td>\n",
       "      <td>187</td>\n",
       "      <td>1329322087849234433</td>\n",
       "      <td>2020-11-19 07:14:18</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Tweets  len  \\\n",
       "0   SICK’s Deep Learning brings simplicity to complex AI ins...  303   \n",
       "1   Google Cloud Debuts Professional Machine Learning Engine...  103   \n",
       "2   Bringing your own custom container image to Amazon SageM...  142   \n",
       "3   Here’s what machines need to understand in order to trul...  238   \n",
       "4   Level up your data science vocabulary: Geometric Distrib...  121   \n",
       "..                                                          ...  ...   \n",
       "95  5 Most Useful Machine Learning Tools every lazy full-sta...  124   \n",
       "96  The way we train AI is fundamentally flawed https://t.co...   67   \n",
       "97  Machine Learning: MLflow 1.12 verbessert die PyTorch-Int...  100   \n",
       "98  Your fleet is in safe hands with #JupiCar, an innovative...  296   \n",
       "99  @Tata_Crucible A. Machine Learning\\n\\n#TechTalk #TataCru...  187   \n",
       "\n",
       "                     ID                Date               Source  Likes  RTs  \n",
       "0   1329334610946260992 2020-11-19 08:04:03              dlvr.it      0    0  \n",
       "1   1329334598455734277 2020-11-19 08:04:00             Paper.li      0    0  \n",
       "2   1329334594085269506 2020-11-19 08:03:59              HubSpot      0    0  \n",
       "3   1329334592449507331 2020-11-19 08:03:59              HubSpot      0    0  \n",
       "4   1329334535532670977 2020-11-19 08:03:45               DeepAI      0    0  \n",
       "..                  ...                 ...                  ...    ...  ...  \n",
       "95  1329322590670893059 2020-11-19 07:16:17             Paper.li      2    1  \n",
       "96  1329322554541154306 2020-11-19 07:16:09  Twitter for Android      3    1  \n",
       "97  1329322407811801088 2020-11-19 07:15:34             Paper.li      0    0  \n",
       "98  1329322279893950464 2020-11-19 07:15:03      Twitter Web App      0    2  \n",
       "99  1329322087849234433 2020-11-19 07:14:18  Twitter for Android      0    0  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We create a pandas dataframe as follows:\n",
    "df = pd.DataFrame(data=[tweet.full_text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "#We add all the information we want to keep about the tweets\n",
    "df['len']  = np.array([len(tweet.full_text) for tweet in tweets])\n",
    "df['ID']   = np.array([tweet.id for tweet in tweets])\n",
    "df['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "df['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "df['Likes']  = np.array([tweet.favorite_count for tweet in tweets])\n",
    "df['RTs']    = np.array([tweet.retweet_count for tweet in tweets])\n",
    "\n",
    "# We display the first 10 elements of the dataframe:\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "display(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the Data Frame it to a csv file so we can read it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets(2302).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First thing would be to get rid off every comma, point and any other strange symbol\n",
    "    - Discuss if @s should be removed or kept\n",
    "    - Discuss if  hastags should be removed\n",
    "    - Remove symbols that stands on their own\n",
    "    - Remove urls\n",
    "- Second thing would be creating the dictionary\n",
    "- Then reducing the dictionay\n",
    "- Finally do some PCA (probabbly(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"@[^\\s]+\", \" \", string)             #remove account tags   \n",
    "    string = re.sub(r\"http[^\\s]+\", \" \", string)          #remove urls\n",
    "    string = re.sub(r\"#[^\\s]+\", \" \", string)             #remove hastags\n",
    "    string = re.sub(r\"[^A-Za-z\\']\", \" \", string)         #remove everything but letters and nummbers(?)\n",
    "    string = re.sub(r\"\\'s\", \" is\", string)               #split 's contractions\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)            #split 's contractions\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)             #split n't contractions\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)             #split 're contractions\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)            #split 'd contractions\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)            #split 'll contractions\n",
    "    string = re.sub(r\"\\'\", \" \", string)                  #remove '\n",
    "    string = re.sub(r\"!\", \" ! \", string)                 #split !\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)               #split ?\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)              #remove more than 1 white space\n",
    "    return string.strip().lower()            #remove start and end white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>len</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Likes</th>\n",
       "      <th>RTs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sick s deep learning brings simplicity to complex ai inspection sick has launched a suite of deep learning apps and services to simplify machine vision quality inspection for challenging food products and agricultural produce especially those that have</td>\n",
       "      <td>303</td>\n",
       "      <td>1329334610946260992</td>\n",
       "      <td>2020-11-19 08:04:03</td>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google cloud debuts professional machine learning engineer certification</td>\n",
       "      <td>103</td>\n",
       "      <td>1329334598455734277</td>\n",
       "      <td>2020-11-19 08:04:00</td>\n",
       "      <td>Paper.li</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bringing your own custom container image to amazon sagemaker studio notebooks</td>\n",
       "      <td>142</td>\n",
       "      <td>1329334594085269506</td>\n",
       "      <td>2020-11-19 08:03:59</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                         Tweets  \\\n",
       "0  sick s deep learning brings simplicity to complex ai inspection sick has launched a suite of deep learning apps and services to simplify machine vision quality inspection for challenging food products and agricultural produce especially those that have   \n",
       "1                                                                                                                                                                                      google cloud debuts professional machine learning engineer certification   \n",
       "2                                                                                                                                                                                 bringing your own custom container image to amazon sagemaker studio notebooks   \n",
       "\n",
       "   len                   ID                 Date    Source  Likes  RTs  \n",
       "0  303  1329334610946260992  2020-11-19 08:04:03   dlvr.it      0    0  \n",
       "1  103  1329334598455734277  2020-11-19 08:04:00  Paper.li      0    0  \n",
       "2  142  1329334594085269506  2020-11-19 08:03:59   HubSpot      0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweets'] = df['Tweets'].apply(clean_str)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify why we did it in count mode and not in binary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet: sick s deep learning brings simplicity to complex ai inspection sick has launched a suite of deep learning apps and services to simplify machine vision quality inspection for challenging food products and agricultural produce especially those that have\n",
      "\n",
      "Vector of the first tweet: [0. 2. 1. ... 0. 0. 0.]\n",
      "\n",
      "Shape of the training set (nb_examples, vector_size): (2302, 7397)\n"
     ]
    }
   ],
   "source": [
    "train_tweets = df['Tweets'].values.tolist()\n",
    "\n",
    "# Create a tokenizer for the nb_words most common words\n",
    "tokenizer = text.Tokenizer()\n",
    "\n",
    "# Build the word index (dictionary)\n",
    "tokenizer.fit_on_texts(train_tweets)\n",
    "\n",
    "# Vectorize texts into one-hot enconding representations\n",
    "train_vectors = tokenizer.texts_to_matrix(train_tweets, mode='count')\n",
    "\n",
    "print('First tweet: ' + train_tweets[0])\n",
    "print('\\nVector of the first tweet: ' + str(train_vectors[0]))\n",
    "print('\\nShape of the training set (nb_examples, vector_size): ' + str(train_vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7396 unique tokens.\n",
      "\n",
      "Show the most frequent word index:\n",
      "   learning (1992) --> 1\n",
      "   machine (1891) --> 2\n",
      "   the (1273) --> 3\n",
      "   to (1242) --> 4\n",
      "   and (1181) --> 5\n",
      "   a (857) --> 6\n",
      "   of (756) --> 7\n",
      "   is (737) --> 8\n",
      "   in (728) --> 9\n",
      "   for (595) --> 10\n",
      "   with (402) --> 11\n",
      "\n",
      "Show the least frequent word index:\n",
      "   simplicity (1) --> 3240\n",
      "   simplify (1) --> 3241\n",
      "   container (1) --> 3242\n",
      "   geometric (1) --> 3243\n",
      "   flexes (1) --> 3244\n",
      "   muscles (1) --> 3245\n",
      "   wasn (1) --> 3246\n",
      "   xataka (1) --> 3247\n",
      "   xico (1) --> 3248\n",
      "   academics (1) --> 3249\n",
      "   ethics (1) --> 3250\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_count = tokenizer.word_counts\n",
    "print(\"There are \" + str(len(word_index)) + \" unique tokens.\\n\")\n",
    "\n",
    "print(\"Show the most frequent word index:\")\n",
    "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=True)):\n",
    "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
    "    if i == 10: \n",
    "        break\n",
    "\n",
    "print(\"\\nShow the least frequent word index:\")\n",
    "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=False)):\n",
    "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
    "    if i == 10: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reducing the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7396 unique words in our dictionary, now we have to decide where to prune the dictionary\n",
    "- Probably we need to remove the **most frequent** articles\n",
    "    - I'll take away the 30 most frequent words (provisional)\n",
    "- We will also remove the **least frequent** words by keeping only the 1000 most frequent words\n",
    "    - We could think about increasing this number to get a better perfromance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_to_remove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5e874bb66efa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Remove the 30 most common words from the vector array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_vectors_reduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_to_remove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Removed the '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_upper_cut\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' most frequent words, with indexes: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_frequent_words_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New shape of the training set (nb_examples, vector_size): '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words_to_remove' is not defined"
     ]
    }
   ],
   "source": [
    "nb_upper_cut = 30\n",
    "nb_words = 1000\n",
    "\n",
    "# Store the word index for the 30 most frequent words\n",
    "words_to_remove_indexes = []\n",
    "words_to_keep_indexes = []\n",
    "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=True)):\n",
    "    if (i <= nb_upper_cut):\n",
    "        words_to_remove_indexes.append(word_index[word])\n",
    "    else:\n",
    "        words_to_keep_indexes.append(word_index[word])\n",
    "    if i == nb_words + nb_upper_cut:\n",
    "        break\n",
    "        \n",
    "# Remove the 30 most common words from the vector array\n",
    "train_vectors_reduced = np.delete(train_vectors, words_to_remove, 1)\n",
    "print('Removed the ' + str(nb_upper_cut) + ' most frequent words, with indexes: ' + str(most_frequent_words_indexes))\n",
    "print('New shape of the training set (nb_examples, vector_size): ' + str(train_vectors_reduced.shape))\n",
    "\n",
    "# Keep only the first 1000 words\n",
    "train_vectors_reduced = train_vectors_reduced[:, words_to_keep_indexes]\n",
    "print('Kept the first ' + str(nb_words) + ' most frequent words, with indexes: ' + str(most_frequent_words_indexes))\n",
    "print('New shape of the training set (nb_examples, vector_size): ' + str(train_vectors_reduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discuss the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
